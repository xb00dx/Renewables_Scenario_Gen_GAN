{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xbgeng/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#The main training function\n",
    "#For event-based scenario generation and spatial scenario generation, implement the code with labels or \n",
    "#reshape the imput samples to spatio-temporal samples respectively.\n",
    "#16 is the maximum value for wind capacity we use. Change to the customized max value for normalized data\n",
    "#import ipdb\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from model import *\n",
    "from util import *\n",
    "from load import load_wind, load_solar_data, load_wind_data_spatial #Change the data source for other tasks\n",
    "from numpy import shape\n",
    "import csv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105120, 52)\n",
      "Maximum value of wind 16.0\n",
      "(105120, 52)\n",
      "Shape TrX (9464, 576)\n",
      "Label shape (9464, 1)\n",
      "shape of training samples  (9464, 576)\n",
      "Training data loaded\n",
      "W_DCGAN model initialized\n",
      "Initializing the generator\n",
      "Input Z shape (32, 100)\n",
      "Input Y shape (32, 5)\n",
      "Z shape (32, 105)\n",
      "h1 shape (32, 1024)\n",
      "h1 shape (32, 1029)\n",
      "h2 shape (32, 4608)\n",
      "h2 shape (32, 6, 6, 128)\n",
      "shape of yb new (32, 6, 6, 5)\n",
      "h2 shape (32, 6, 6, 133)\n",
      "h3 shape (32, 12, 12, 64)\n",
      "h3 shape (32, 12, 12, 69)\n",
      "Initializing the discriminator\n",
      "Y shape (32, 5)\n",
      "image shape (32, 24, 24, 1)\n",
      "yb shape (32, 1, 1, 5)\n",
      "X shape (32, 24, 24, 6)\n",
      "h1 shape (32, 12, 12, 64)\n",
      "h1 shape (32, 12, 12, 69)\n",
      "h2 shape (32, 6, 6, 128)\n",
      "discri shape (32, 1024)\n",
      "Initializing the discriminator\n",
      "Y shape (32, 5)\n",
      "image shape (32, 24, 24, 1)\n",
      "yb shape (32, 1, 1, 5)\n",
      "X shape (32, 24, 24, 6)\n",
      "h1 shape (32, 12, 12, 64)\n",
      "h1 shape (32, 12, 12, 69)\n",
      "h2 shape (32, 6, 6, 128)\n",
      "discri shape (32, 1024)\n",
      "WARNING:tensorflow:From /home/xbgeng/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/xbgeng/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 70 #Number of overall training epochs on training data\n",
    "learning_rate = 0.0002 \n",
    "batch_size = 32\n",
    "image_shape = [24,24,1] #The shape for input data\n",
    "dim_z = 100 #input dimension for samples\n",
    "dim_W1 = 1024 #first layer neurons\n",
    "dim_W2 = 128 #second layer neurons\n",
    "dim_W3 = 64 #third layer#16 is the maximum value for wind capacity we use. Change to your max value here\n",
    "dim_channel = 1 #reserved for future use if multi=channels\n",
    "mu, sigma = 0, 0.1 # input Gaussian\n",
    "events_num=5 #kind of events\n",
    "\n",
    "visualize_dim=32\n",
    "generated_dim=32\n",
    "\n",
    "\n",
    "#Comment out corresponding part to reproduce the results for \n",
    "#wind_events_generation, solar_events_generation, spatial_generation respectively\n",
    "trX, trY=load_wind()\n",
    "#trX, trY=load_solar()\n",
    "#trX, trY=load_spatial()\n",
    "\n",
    "print(\"shape of training samples \", shape(trX))\n",
    "print(\"Training data loaded\")\n",
    "\n",
    "dcgan_model = GAN(\n",
    "    dim_y=events_num # Change parameters based on number of events\n",
    "    #change paprameters here for model revision\n",
    "    #dim_z: the dimension for input noise\n",
    "    #W1,W2,W3: the dimension for convolutional layers\n",
    "        )\n",
    "print(\"W_DCGAN model initialized\")\n",
    "\n",
    "\n",
    "#Z_tf,Y_tf: placeholder\n",
    "#image_tf: image placeholder\n",
    "#d_cost_tf, g_cost_tf: discriminator and generator cost#16 is the maximum value for wind capacity we use. Change to your max value here\n",
    "#p_real, p_gen: the output of discriminator to judge real/generated\n",
    "Z_tf, Y_tf, image_tf, d_cost_tf, g_cost_tf, p_real, p_gen = dcgan_model.build_model()\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "discrim_vars = filter(lambda x: x.name.startswith('discrim'), tf.trainable_variables())\n",
    "gen_vars = filter(lambda x: x.name.startswith('gen'), tf.trainable_variables())\n",
    "discrim_vars = [i for i in discrim_vars]\n",
    "gen_vars = [i for i in gen_vars]\n",
    "\n",
    "train_op_discrim = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(-d_cost_tf, var_list=discrim_vars))\n",
    "train_op_gen = (tf.train.RMSPropOptimizer(learning_rate=1e-4).minimize(g_cost_tf, var_list=gen_vars))\n",
    "\n",
    "Z_tf_sample, Y_tf_sample, image_tf_sample = dcgan_model.samples_generator(batch_size=visualize_dim)\n",
    "tf.initialize_all_variables().run()\n",
    "\n",
    "Zs = np.random.normal(mu, sigma, size=[batch_size, dim_z]).astype(np.float32)\n",
    "Y_np_sample = OneHot(np.random.randint(events_num, size=[visualize_dim]), n=events_num)\n",
    "iterations = 0\n",
    "k = 4 #control the balance of training D and G\n",
    "\n",
    "gen_loss_all=[]#16 is the maximum value for wind capacity we use. Change to your max value here\n",
    "P_real=[]\n",
    "P_fake=[]\n",
    "P_distri=[]\n",
    "discrim_loss=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0\n",
      "iterations  0\n",
      "Average P(real)= 0.33327425\n",
      "Average P(gen)= 0.33468157\n",
      "Discrim loss: -46.115234\n",
      "epoch1\n",
      "epoch2\n",
      "epoch3\n",
      "iterations  1000\n",
      "Average P(real)= 0.36257443\n",
      "Average P(gen)= 0.26104903\n",
      "Discrim loss: 3326.7842\n",
      "epoch4\n",
      "epoch5\n",
      "epoch6\n",
      "iterations  2000\n",
      "Average P(real)= 0.37467223\n",
      "Average P(gen)= 0.27132493\n",
      "Discrim loss: 3386.4844\n",
      "epoch7\n",
      "epoch8\n",
      "epoch9\n",
      "epoch10\n",
      "iterations  3000\n",
      "Average P(real)= 0.38259315\n",
      "Average P(gen)= 0.27625382\n",
      "Discrim loss: 3484.5273\n",
      "epoch11\n",
      "epoch12\n"
     ]
    }
   ],
   "source": [
    "#begin training\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch\" + str(epoch))\n",
    "    index = np.arange(len(trY))\n",
    "    np.random.shuffle(index)\n",
    "    trX = trX[index]\n",
    "    trY = trY[index]\n",
    "    trY2 = OneHot(trY, n=events_num)\n",
    "\n",
    "    for start, end in zip(\n",
    "            range(0, len(trY), batch_size),\n",
    "            range(batch_size, len(trY), batch_size)\n",
    "            ):\n",
    "\n",
    "        Xs = trX[start:end].reshape([-1, 24, 24, 1])\n",
    "        Ys = trY2[start:end]\n",
    "\n",
    "        #use uniform or Gaussian distribution data to generate adversarial samples\n",
    "        Zs = np.random.normal(mu, sigma, size=[batch_size, dim_z]).astype(np.float32)\n",
    "\n",
    "        #for each iteration, generate g and d respectively, k=2\n",
    "        if np.mod( iterations, k) == 0:\n",
    "            _, gen_loss_val = sess.run(\n",
    "                    [train_op_gen, g_cost_tf],\n",
    "                    feed_dict={\n",
    "                        Z_tf:Zs,\n",
    "                        Y_tf:Ys,\n",
    "                        image_tf: Xs\n",
    "                        })\n",
    "            discrim_loss_val, p_real_val, p_gen_val = sess.run([d_cost_tf,p_real,p_gen], feed_dict={Z_tf:Zs, image_tf:Xs, Y_tf:Ys})\n",
    "\n",
    "            '''print(\"=========== updating G ==========\")\n",
    "            print(\"iteration:\", iterations)\n",
    "            print(\"gen loss:\", gen_loss_val)\n",
    "            print(\"discrim loss:\", discrim_loss_val)'''\n",
    "\n",
    "        else:\n",
    "            _, discrim_loss_val = sess.run(\n",
    "                    [train_op_discrim, d_cost_tf],\n",
    "                    feed_dict={\n",
    "                        Z_tf:Zs,\n",
    "                        Y_tf:Ys,\n",
    "                        image_tf:Xs\n",
    "                        })\n",
    "\n",
    "            '''print(\"=========== updating D ==========\")\n",
    "            print(\"iteration:\", iterations)\n",
    "            print(\"gen loss:\", gen_loss_val)\n",
    "            print(\"discrim loss:\", discrim_loss_val)'''\n",
    "\n",
    "            gen_loss_val, p_real_val, p_gen_val = sess.run([g_cost_tf, p_real, p_gen],\n",
    "                                                       feed_dict={Z_tf: Zs, image_tf: Xs, Y_tf: Ys})\n",
    "        P_real.append(p_real_val.mean())\n",
    "        P_fake.append(p_gen_val.mean())\n",
    "        '''gen_loss_val, p_real_val, p_gen_val = sess.run([g_cost_tf, p_real, p_gen],\n",
    "                                                       feed_dict={Z_tf: Zs, image_tf: fake_data, Y_tf: Ys})'''\n",
    "        discrim_loss.append(discrim_loss_val)\n",
    "\n",
    "\n",
    "        if np.mod(iterations, 1000) == 0:\n",
    "            print(\"iterations \", iterations)\n",
    "            print(\"Average P(real)=\", p_real_val.mean())\n",
    "            print(\"Average P(gen)=\", p_gen_val.mean())\n",
    "            print(\"Discrim loss:\", discrim_loss_val)\n",
    "            Y_np_sample = OneHot(np.random.randint(5, size=[visualize_dim]), n=events_num)\n",
    "            Z_np_sample = np.random.normal(mu, sigma, size=[batch_size, dim_z]).astype(np.float32)\n",
    "            generated_samples = sess.run(\n",
    "                image_tf_sample,\n",
    "                feed_dict={\n",
    "                    Z_tf_sample: Z_np_sample,\n",
    "                    Y_tf_sample: Y_np_sample\n",
    "                })\n",
    "            generated_samples=generated_samples.reshape([-1,576])\n",
    "            generated_samples = generated_samples * 16 #16 is the maximum value for wind capacity we use. Change to your max value here\n",
    "            # csvfile=file('%s.csv' %iterations, 'wb')\n",
    "            csvfile=open('%s.csv' %iterations, 'w')\n",
    "            writer=csv.writer(csvfile)\n",
    "            writer.writerows(generated_samples)\n",
    "\n",
    "        iterations += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_np_sample = OneHot(np.random.randint(5, size=[visualize_dim]), n=events_num) \n",
    "Zs = np.random.normal(mu, sigma, size=[batch_size, dim_z]).astype(np.float32)\n",
    "generated_samples = sess.run(\n",
    "    image_tf_sample,\n",
    "    feed_dict={\n",
    "        Z_tf_sample: Z_np_sample,\n",
    "        Y_tf_sample: Y_np_sample\n",
    "    })\n",
    "generated_samples=generated_samples.reshape([-1,576])\n",
    "generated_samples = generated_samples * 16 #16 is the maximum value for wind capacity we use. Change to your max value here\n",
    "# csvfile=file('sample1.csv', 'wb')\n",
    "csvfile=open('sample1.csv', 'wb')\n",
    "writer=csv.writer(csvfile)\n",
    "writer.writerows(generated_samples)\n",
    "# csvfile=file('label1.csv', 'wb')\n",
    "csvfile=open('label1.csv', 'wb')\n",
    "writer=csv.writer(csvfile)\n",
    "writer.writerows(Y_np_sample)\n",
    "\n",
    "\n",
    "#plot the loss and P_real as well as P_fake\n",
    "print(\"P_real\",P_real)\n",
    "print(\"P_fake\",P_fake)\n",
    "\n",
    "plt.plot(P_real,label=\"real\")\n",
    "plt.plot(P_fake,label=\"fake\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(discrim_loss,label=\"discrim_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
